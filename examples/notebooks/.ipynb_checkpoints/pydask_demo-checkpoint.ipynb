{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "697f7e53",
   "metadata": {},
   "source": [
    "# Distributed Data Processing with DaskProcessor\n",
    "\n",
    "This notebook demonstrates how to use **DaskProcessor** for scalable parallel processing of ROOT data files using Dask's distributed computing framework.\n",
    "\n",
    "## Overview\n",
    "\n",
    "DaskProcessor provides a drop-in replacement for the standard Processor that uses Dask for:\n",
    "- **Local parallelization**: Efficiently use all cores on a single machine\n",
    "- **Distributed computing**: Connect to Dask clusters for large-scale processing\n",
    "- **Progress monitoring**: Track processing progress across multiple files\n",
    "- **Error resilience**: Built-in retry mechanisms for failed tasks\n",
    "\n",
    "Key advantages:\n",
    "- Same output format (Awkward Arrays) as standard Processor\n",
    "- Easy switching between serial, parallel, and distributed modes\n",
    "- Seamless integration with HPC clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36924d6f",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac36f622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import time\n",
    "import os\n",
    "from typing import Dict, Optional\n",
    "\n",
    "# External packages\n",
    "import awkward as ak\n",
    "\n",
    "# pyutils imports\n",
    "from pyutils.pydask import DaskProcessor\n",
    "from pyutils.pyprocess import Processor\n",
    "from pyutils.pylogger import Logger\n",
    "\n",
    "# Initialize logger for notebook demonstrations\n",
    "logger = Logger(print_prefix=\"[pydask demo]\", verbosity=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c0d172",
   "metadata": {},
   "source": [
    "## 2. Initialize DaskProcessor\n",
    "\n",
    "Create and configure a DaskProcessor instance. DaskProcessor shares the same constructor parameters as the standard Processor but uses Dask for distributed execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3ecc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DaskProcessor with remote settings\n",
    "dask_proc = DaskProcessor(\n",
    "    tree_path=\"EventNtuple/ntuple\",\n",
    "    use_remote=True,           # Access remote persistent datasets\n",
    "    location=\"disk\",           # Read from disk storage\n",
    "    verbosity=1,\n",
    "    worker_verbosity=0\n",
    ")\n",
    "\n",
    "logger.log(\"DaskProcessor initialized with remote settings:\", \"success\")\n",
    "logger.log(f\"  tree_path: {dask_proc._base.tree_path}\", \"info\")\n",
    "logger.log(f\"  use_remote: {dask_proc._base.use_remote}\", \"info\")\n",
    "logger.log(f\"  location: {dask_proc._base.location}\", \"info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e1e64a",
   "metadata": {},
   "source": [
    "## 3. Single File Processing\n",
    "\n",
    "Process a single ROOT file using DaskProcessor. This demonstrates the basic API for specifying branches and output field names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415dae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Single file processing\n",
    "# (Replace with actual file path and branches for real data)\n",
    "\n",
    "branches = {\n",
    "    \"trk.mom\": \"momentum\",  # Map ROOT branch to output field\n",
    "    \"trk.pos\": \"position\"   # Multiple branches can be selected\n",
    "}\n",
    "\n",
    "# This would process a single file:\n",
    "# result = dask_proc.process_data(\n",
    "#     file_name=\"/path/to/data.root\",\n",
    "#     branches=branches\n",
    "# )\n",
    "\n",
    "logger.log(\"Single File Processing Example:\", \"info\")\n",
    "logger.log(\"When you have a single file, use file_name parameter:\", \"info\")\n",
    "logger.log(\"\"\"\n",
    "result = dask_proc.process_data(\n",
    "    file_name=\"/path/to/data.root\",\n",
    "    branches={\"trk.mom\": \"momentum\", \"trk.pos\": \"position\"},\n",
    "    show_progress=True\n",
    ")\n",
    "The result is an Awkward Array with the selected branches.\n",
    "\"\"\", \"info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e1e81e",
   "metadata": {},
   "source": [
    "## 4. Multi-File Processing with File Lists\n",
    "\n",
    "Process multiple files from a file list. This is where DaskProcessor shows its power by distributing work across multiple workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47d5045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MDS3a.txt file list from the repository\n",
    "import os\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "file_list_path = os.path.join(notebook_dir, \"../../MDS3a.txt\")\n",
    "\n",
    "# Read the file list\n",
    "if os.path.exists(file_list_path):\n",
    "    with open(file_list_path, 'r') as f:\n",
    "        sample_files = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    logger.log(f\"Loaded MDS3a.txt file list\", \"success\")\n",
    "    logger.log(f\"Total files available: {len(sample_files)}\", \"info\")\n",
    "    \n",
    "    # Display statistics\n",
    "    logger.log(\"\\nFile list statistics:\", \"info\")\n",
    "    logger.log(f\"  First file: {sample_files[0].split('/')[-1]}\", \"info\")\n",
    "    logger.log(f\"  Last file: {sample_files[-1].split('/')[-1]}\", \"info\")\n",
    "    logger.log(f\"  Dataset: MDC2025-001\", \"info\")\n",
    "else:\n",
    "    logger.log(\"MDS3a.txt not found - using demo file paths\", \"warning\")\n",
    "    sample_files = [\n",
    "        \"/pnfs/mu2e/persistent/datasets/phy-nts/nts/mu2e/ensembleMDS3aMix1BBTriggered/MDC2025-001/root/01/18/nts.mu2e.ensembleMDS3aMix1BBTriggered.MDC2025-001.001430_00000552.root\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c67fc03",
   "metadata": {},
   "source": [
    "## 5. Configure Parallelization Parameters\n",
    "\n",
    "Explore key parameters for optimizing DaskProcessor performance based on your hardware and workload characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b08e198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key DaskProcessor.process_data() parameters:\n",
    "\n",
    "parameter_guide = {\n",
    "    \"n_workers\": {\n",
    "        \"description\": \"Number of parallel workers in the Dask cluster\",\n",
    "        \"default\": \"All available CPU cores\",\n",
    "        \"example\": \"n_workers=4  # Use 4 workers\",\n",
    "        \"use_case\": \"Scales to machine resources\"\n",
    "    },\n",
    "    \"threads_per_worker\": {\n",
    "        \"description\": \"Number of threads per worker\",\n",
    "        \"default\": 1,\n",
    "        \"example\": \"threads_per_worker=2  # 2 threads per worker\",\n",
    "        \"use_case\": \"For I/O-bound tasks, increase threads\"\n",
    "    },\n",
    "    \"processes\": {\n",
    "        \"description\": \"Use processes instead of threads\",\n",
    "        \"default\": False,\n",
    "        \"example\": \"processes=True  # Use process-based parallelism\",\n",
    "        \"use_case\": \"CPU-bound tasks, GIL avoidance\"\n",
    "    },\n",
    "    \"show_progress\": {\n",
    "        \"description\": \"Display a progress bar during processing\",\n",
    "        \"default\": True,\n",
    "        \"example\": \"show_progress=True\",\n",
    "        \"use_case\": \"Monitor long-running jobs\"\n",
    "    },\n",
    "    \"retries\": {\n",
    "        \"description\": \"Number of retries for failed tasks\",\n",
    "        \"default\": 0,\n",
    "        \"example\": \"retries=2  # Retry up to 2 times\",\n",
    "        \"use_case\": \"Resilience to transient failures\"\n",
    "    }\n",
    "}\n",
    "\n",
    "logger.log(\"Parallelization Parameter Guide:\", \"success\")\n",
    "for param, details in parameter_guide.items():\n",
    "    logger.log(f\"\\n{param}:\", \"info\")\n",
    "    logger.log(f\"  Description: {details['description']}\", \"info\")\n",
    "    logger.log(f\"  Default: {details['default']}\", \"info\")\n",
    "    logger.log(f\"  Example: {details['example']}\", \"info\")\n",
    "    logger.log(f\"  Use Case: {details['use_case']}\", \"info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d1cb73",
   "metadata": {},
   "source": [
    "## 6. Connect to Remote Dask Scheduler\n",
    "\n",
    "Scale beyond a single machine by connecting to a Dask cluster for large-scale distributed computing across multiple nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4814447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remote scheduler setup instruction\n",
    "remote_scheduler_guide = \"\"\"\n",
    "STEP 1: Start Dask Scheduler on Cluster Head Node\n",
    "===================================================\n",
    "In a tmux session on your Dask cluster's head node:\n",
    "\n",
    "    dask-scheduler --port 8786\n",
    "\n",
    "The scheduler will output something like:\n",
    "    \"Scheduler started at tcp://cluster-head.example.com:8786\"\n",
    "\n",
    "\n",
    "STEP 2: Connect from Analysis Script\n",
    "====================================\n",
    "In your analysis notebook/script:\n",
    "\n",
    "    branches = {\"trk.mom\": \"momentum\", \"trk.pos\": \"position\"}\n",
    "    \n",
    "    result = dask_proc.process_data(\n",
    "        file_list_path=\"/path/to/file_list.txt\",\n",
    "        branches=branches,\n",
    "        scheduler_address=\"tcp://cluster-head.example.com:8786\"\n",
    "    )\n",
    "\n",
    "The scheduler handles distributing tasks across all available worker nodes.\n",
    "Worker processes should already be running on cluster nodes.\n",
    "\n",
    "\n",
    "ADVANTAGES OF REMOTE SCHEDULER:\n",
    "- Distribute work across multiple machines\n",
    "- Utilize all available compute resources on the cluster\n",
    "- Persistent scheduler can accept multiple jobs\n",
    "- Progress monitoring from your local machine\n",
    "- Decoupled job submission from execution\n",
    "\"\"\"\n",
    "\n",
    "logger.log(remote_scheduler_guide, \"info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6e5cbc",
   "metadata": {},
   "source": [
    "## 7. Custom Worker Functions\n",
    "\n",
    "Define custom processing logic for specialized data transformations, filtering, and manipulations that go beyond simple branch selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e43e725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example custom worker function\n",
    "def custom_physics_worker(file_path: str) -> Optional[ak.Array]:\n",
    "    \"\"\"\n",
    "    Custom worker function that performs specialized physics analysis:\n",
    "    - Reads specific branches from ROOT file\n",
    "    - Applies physics cuts (e.g., momentum threshold)\n",
    "    - Returns filtered Awkward Array\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the ROOT file to process\n",
    "    \n",
    "    Returns:\n",
    "        ak.Array or None: Filtered data as Awkward Array\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from pyutils.pyread import Reader\n",
    "        \n",
    "        # Initialize reader for this file\n",
    "        reader = Reader(file_path, tree_path=\"EventNtuple/ntuple\")\n",
    "        \n",
    "        # Read track data\n",
    "        data = reader.read(branches=[\"trk.mom\", \"trk.pos\", \"trk.charge\"])\n",
    "        \n",
    "        if data is None or len(data) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Example physics cuts:\n",
    "        # - Keep only high-momentum tracks (momentum > 100 MeV/c)\n",
    "        # - Keep only positively charged tracks\n",
    "        high_mom_mask = data.trk.mom > 100\n",
    "        positive_charge_mask = data.trk.charge > 0\n",
    "        \n",
    "        filtered = data[high_mom_mask & positive_charge_mask]\n",
    "        \n",
    "        return filtered if len(filtered) > 0 else None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Example: Using custom worker function\n",
    "logger.log(\"Custom Worker Function Example:\", \"success\")\n",
    "logger.log(\"\"\"\n",
    "# Define your custom worker function\n",
    "\n",
    "def my_custom_worker(file_path):\n",
    "    from pyutils.pyread import Reader\n",
    "    reader = Reader(file_path, tree_path=\"EventNtuple/ntuple\")\n",
    "    data = reader.read(branches=[\"trk.mom\", \"trk.pos\"])\n",
    "    \n",
    "    # Apply custom cuts\n",
    "    filtered = data[data.trk.mom > 100]\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "# Use it with DaskProcessor\n",
    "result = dask_proc.process_data(\n",
    "    file_list_path=\"/path/to/file_list.txt\",\n",
    "    custom_worker_func=my_custom_worker,\n",
    "    n_workers=4,\n",
    "    show_progress=True\n",
    ")\n",
    "\"\"\", \"info\")\n",
    "\n",
    "logger.log(\"\\nKey Requirements for Custom Worker Functions:\", \"info\")\n",
    "logger.log(\"  ✓ Must accept file_path as a string parameter\", \"info\")\n",
    "logger.log(\"  ✓ Must return an Awkward Array or None\", \"info\")\n",
    "logger.log(\"  ✓ Should handle exceptions gracefully\", \"info\")\n",
    "logger.log(\"  ✓ Cannot rely on global state (runs in separate processes)\", \"info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3453a50",
   "metadata": {},
   "source": [
    "## 8. Compare Standard vs Dask Processors\n",
    "\n",
    "Benchmark and understand the performance differences between standard Processor and DaskProcessor for various workload scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9ef82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "comparison = {\n",
    "    \"Feature\": [\n",
    "        \"Parallelization Type\",\n",
    "        \"Best For\",\n",
    "        \"Scalability\",\n",
    "        \"Cluster Support\",\n",
    "        \"Progress Tracking\",\n",
    "        \"Memory Efficiency\",\n",
    "        \"Debugging\",\n",
    "        \"Error Resilience\"\n",
    "    ],\n",
    "    \"Standard Processor\": [\n",
    "        \"Thread pool (max_workers param)\",\n",
    "        \"Small file lists, quick testing\",\n",
    "        \"Single machine (limited by cores)\",\n",
    "        \"No native support\",\n",
    "        \"Basic logging only\",\n",
    "        \"Loads all results in memory\",\n",
    "        \"Easier (all in-process)\",\n",
    "        \"Limited retry mechanism\"\n",
    "    ],\n",
    "    \"DaskProcessor\": [\n",
    "        \"Dask distributed framework\",\n",
    "        \"Large file lists, HPC clusters\",\n",
    "        \"Multi-machine via remote scheduler\",\n",
    "        \"Full support (tcp:// addresses)\",\n",
    "        \"Progress bar with live updates\",\n",
    "        \"Distributed memory management\",\n",
    "        \"More complex (separate processes)\",\n",
    "        \"Built-in retries parameter\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Display comparison\n",
    "logger.log(\"STANDARD PROCESSOR vs DASK PROCESSOR\", \"success\")\n",
    "logger.log(\"=\" * 80, \"info\")\n",
    "\n",
    "import pandas as pd\n",
    "df_comparison = pd.DataFrame(comparison)\n",
    "logger.log(str(df_comparison.to_string(index=False)), \"info\")\n",
    "\n",
    "# When to use which\n",
    "logger.log(\"\\n\" + \"=\" * 80, \"info\")\n",
    "logger.log(\"DECISION GUIDE:\", \"success\")\n",
    "logger.log(\"\"\"\n",
    "Use STANDARD PROCESSOR when:\n",
    "  • Processing a small number of files (< 10-20)\n",
    "  • You want simpler debugging\n",
    "  • Operating on a single machine without HPC resources\n",
    "  • Throughput is not critical\n",
    "  • You're doing interactive analysis (faster startup)\n",
    "\n",
    "Use DASK PROCESSOR when:\n",
    "  • Processing hundreds or thousands of files\n",
    "  • You have access to a multi-core machine or HPC cluster\n",
    "  • You need progress monitoring for long-running jobs\n",
    "  • You want resilience to transient failures (retries)\n",
    "  • Complex workflows requiring custom worker functions\n",
    "  • You want easy scaling without code changes\n",
    "\n",
    "KEY ADVANTAGE: Both have identical APIs and output formats!\n",
    "You can switch between them without changing analysis code.\n",
    "\"\"\", \"info\")\n",
    "\n",
    "logger.log(\"\\n✓ This concludes the DaskProcessor tutorial!\", \"success\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
