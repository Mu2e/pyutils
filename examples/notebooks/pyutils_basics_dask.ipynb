{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "750b4f5c",
   "metadata": {},
   "source": [
    "# Getting Started: pyutils Basics with Dask\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial builds on the basic `pyutils` functionality from [pyutils_basics.ipynb](pyutils_basics.ipynb) but focuses on **parallel processing** of multiple files using **DaskProcessor**.\n",
    "\n",
    "You will learn how to:\n",
    "- Use `DaskProcessor` to process multiple files in parallel\n",
    "- Apply the same analysis to aggregated data from many files\n",
    "- Scale from a single file to thousands of files without code changes\n",
    "- Leverage multi-core systems and HPC clusters\n",
    "\n",
    "## Key Difference from Serial Processing\n",
    "\n",
    "| Aspect | Standard `Processor` | `DaskProcessor` |\n",
    "|--------|----------------------|-----------------|\n",
    "| **Best For** | Single file or small file lists | Multiple files / large datasets |\n",
    "| **Parallelization** | Thread pool on single machine | Dask distributed framework |\n",
    "| **Scaling** | Limited to machine cores | Scales to clusters |\n",
    "| **Progress Tracking** | Basic logging | Real-time progress bar |\n",
    "\n",
    "## Table of Contents\n",
    "1. Setting up your environment\n",
    "2. Creating a file list for multi-file processing\n",
    "3. Processing data with DaskProcessor\n",
    "4. Applying selection cuts to aggregated data\n",
    "5. Inspecting aggregated data\n",
    "6. Performing vector operations\n",
    "7. Creating plots from aggregated results\n",
    "8. Complete example with multiple files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ae7651",
   "metadata": {},
   "source": [
    "## 1. Setting Up Your Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c2364e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import external packages\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "\n",
    "# Import pyutils modules\n",
    "from pyutils.pydask import DaskProcessor\n",
    "from pyutils.pyselect import Select\n",
    "from pyutils.pyprint import Print\n",
    "from pyutils.pyvector import Vector\n",
    "from pyutils.pyplot import Plot\n",
    "from pyutils.pylogger import Logger\n",
    "\n",
    "# Initialize logger for notebook output\n",
    "logger = Logger(print_prefix=\"[pyutils_dask]\", verbosity=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8066b44f",
   "metadata": {},
   "source": [
    "## 2. Creating a File List for Multi-File Processing\n",
    "\n",
    "When working with multiple files, you need to create a file list. Each line in the file should contain the full path to a ROOT file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e337bf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load the MDS3a.txt file list from the repository\n",
    "notebook_dir = os.getcwd()\n",
    "file_list_path = os.path.join(notebook_dir, \"../../MDS3a.txt\")\n",
    "\n",
    "# Read the file list\n",
    "if os.path.exists(file_list_path):\n",
    "    with open(file_list_path, 'r') as f:\n",
    "        sample_files = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    logger.log(f\"Loaded file list from: MDS3a.txt\", \"success\")\n",
    "    logger.log(f\"Total files available: {len(sample_files)}\", \"info\")\n",
    "else:\n",
    "    logger.log(\"MDS3a.txt not found, creating empty list\", \"warning\")\n",
    "    sample_files = []\n",
    "\n",
    "# Display first few files\n",
    "logger.log(\"First 3 files in the list:\", \"info\")\n",
    "for i, f in enumerate(sample_files[:3]):\n",
    "    logger.log(f\"  {i+1}. {f.split('/')[-1]}\", \"info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12260f2d",
   "metadata": {},
   "source": [
    "## 3. Processing Data with DaskProcessor\n",
    "\n",
    "Now process all files in parallel using DaskProcessor. This demonstrates the key advantage of Dask: as your file list grows from 1 to 1000s of files, the same code automatically scales across available cores or connects to a remote HPC cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1f22bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DaskProcessor with remote settings for multi-file processing\n",
    "processor = DaskProcessor(\n",
    "    tree_path=\"EventNtuple/ntuple\",\n",
    "    use_remote=True,           # Access remote persistent datasets\n",
    "    location=\"disk\",           # Read from disk storage\n",
    "    verbosity=1,\n",
    "    worker_verbosity=0\n",
    ")\n",
    "\n",
    "logger.log(\"DaskProcessor initialized\", \"success\")\n",
    "logger.log(\"Ready to process multiple files in parallel\", \"info\")\n",
    "\n",
    "# Define branches to extract\n",
    "branches = [\"trksegs\"]\n",
    "\n",
    "logger.log(f\"Branches to extract: {branches}\", \"info\")\n",
    "logger.log(\"Reading from persistent dataset (disk location)\", \"info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbd14c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data using DaskProcessor with multiple files in parallel\n",
    "logger.log(\"Processing multiple files with DaskProcessor...\", \"info\")\n",
    "logger.log(f\"Total files to process: {len(sample_files)}\", \"info\")\n",
    "\n",
    "try:\n",
    "    if sample_files and file_list_path:\n",
    "        logger.log(\"Using DaskProcessor with multi-file parallel processing\", \"info\")\n",
    "        \n",
    "        # Process all files in parallel\n",
    "        data = processor.process_data(\n",
    "            file_list_path=file_list_path,\n",
    "            branches=branches,\n",
    "            n_workers=4,              # Use 4 parallel workers\n",
    "            threads_per_worker=1,     # 1 thread per worker\n",
    "            processes=False,          # Use threads (not processes)\n",
    "            show_progress=True        # Show progress bar during processing\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        logger.log(\"File list not available, processing single file for demo\", \"warning\")\n",
    "        \n",
    "        from pyutils.pyprocess import Processor\n",
    "        demo_processor = Processor(\n",
    "            verbosity=1,\n",
    "            use_remote=True,\n",
    "            location=\"disk\"\n",
    "        )\n",
    "        \n",
    "        # Use first file from MDS3a.txt list\n",
    "        if sample_files:\n",
    "            demo_file = sample_files[0]\n",
    "            logger.log(f\"Processing first file from MDS3a.txt: {demo_file.split('/')[-1]}\", \"info\")\n",
    "        else:\n",
    "            demo_file = \"/pnfs/mu2e/tape/phy-nts/nts/mu2e/MDS2ac-OnSpillTriggered/MDC2020aw_perfect_v1_3/root/8c/0b/nts.mu2e.MDS2ac-OnSpillTriggered.MDC2020aw_perfect_v1_3.0.root\"\n",
    "            logger.log(\"Using fallback file for demo\", \"warning\")\n",
    "        \n",
    "        data = demo_processor.process_data(\n",
    "            file_name=demo_file,\n",
    "            branches=branches\n",
    "        )\n",
    "    \n",
    "    logger.log(\"Data aggregation complete\", \"success\")\n",
    "    logger.log(f\"Total events from all files: {len(data)}\", \"info\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.log(f\"Error during processing: {e}\", \"error\")\n",
    "    logger.log(\"Check that Mu2e environment is properly initialized\", \"warning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050d8435",
   "metadata": {},
   "source": [
    "## 4. Applying Selection Cuts to Aggregated Data\n",
    "\n",
    "After processing all files with DaskProcessor, apply selection cuts to the combined dataset. The workflow is identical to the serial case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75402889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize selector\n",
    "selector = Select(verbosity=1)\n",
    "\n",
    "# Select track segments at tracker entrance\n",
    "logger.log(\"Selecting tracks at tracker entrance...\", \"info\")\n",
    "\n",
    "at_trkent = selector.select_surface(\n",
    "    data=data,\n",
    "    surface_name=\"TT_Front\"\n",
    ")\n",
    "\n",
    "# Add selection mask to data\n",
    "data[\"at_trkent\"] = at_trkent\n",
    "\n",
    "# Apply mask\n",
    "trkent = data[at_trkent]\n",
    "\n",
    "logger.log(f\"Selected {len(trkent)} events at tracker entrance\", \"success\")\n",
    "logger.log(f\"Selection efficiency: {100*len(trkent)/len(data):.1f}%\", \"info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e82415f",
   "metadata": {},
   "source": [
    "## 5. Inspecting Aggregated Data\n",
    "\n",
    "Verify the data structure and cuts using the Print utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a9762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize printer\n",
    "printer = Print(verbose=False)\n",
    "\n",
    "# Display data structure after selection\n",
    "logger.log(\"Data structure at tracker entrance:\", \"info\")\n",
    "printer.print_n_events(trkent, n_events=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea902fd",
   "metadata": {},
   "source": [
    "## 6. Performing Vector Operations on Aggregated Data\n",
    "\n",
    "Calculate quantities (like momentum magnitude) from the aggregated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cce3f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vector processor\n",
    "vector = Vector(verbosity=1)\n",
    "\n",
    "# Calculate momentum magnitude for all tracks\n",
    "logger.log(\"Computing momentum magnitude...\", \"info\")\n",
    "\n",
    "mom_mag = vector.get_mag(\n",
    "    branch=trkent[\"trksegs\"],\n",
    "    vector_name=\"mom\"\n",
    ")\n",
    "\n",
    "logger.log(\"Momentum magnitude computed\", \"success\")\n",
    "logger.log(f\"Momentum range: {ak.min(ak.flatten(mom_mag, axis=None)):.2f} - {ak.max(ak.flatten(mom_mag, axis=None)):.2f} MeV/c\", \"info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b8b6ae",
   "metadata": {},
   "source": [
    "## 7. Creating Plots from Aggregated Results\n",
    "\n",
    "Create publication-quality plots from the combined dataset. These plots show aggregated statistics across all processed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2714dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize plotter\n",
    "plotter = Plot()\n",
    "\n",
    "# Flatten arrays for plotting\n",
    "time_flat = ak.flatten(trkent[\"trksegs\"][\"time\"], axis=None)\n",
    "mom_mag_flat = ak.flatten(mom_mag, axis=None)\n",
    "\n",
    "logger.log(f\"Preparing plots from {len(time_flat)} track measurements\", \"info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a2518f",
   "metadata": {},
   "source": [
    "### 7.1 Create 1D Histogram: Time Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e722c894",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.25' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/bin/python -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "logger.log(\"Creating 1D histogram of time distribution...\", \"info\")\n",
    "\n",
    "plotter.plot_1D(\n",
    "    time_flat,\n",
    "    nbins=100,\n",
    "    xmin=450,\n",
    "    xmax=1695,\n",
    "    title=\"Time at Tracker Entrance (from Dask-processed Data)\",\n",
    "    xlabel=\"Fit time at Trk Ent [ns]\",\n",
    "    ylabel=\"Events per bin\",\n",
    "    out_path='h1_time_dask.png',\n",
    "    stat_box=True,\n",
    "    error_bars=True\n",
    ")\n",
    "\n",
    "logger.log(\"1D histogram created: h1_time_dask.png\", \"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b028bda6",
   "metadata": {},
   "source": [
    "### 7.2 Create 2D Histogram: Momentum vs. Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f696ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.log(\"Creating 2D histogram of momentum vs. time...\", \"info\")\n",
    "\n",
    "plotter.plot_2D(\n",
    "    x=mom_mag_flat,\n",
    "    y=time_flat,\n",
    "    nbins_x=100,\n",
    "    xmin=85,\n",
    "    xmax=115,\n",
    "    nbins_y=100,\n",
    "    ymin=450,\n",
    "    ymax=1650,\n",
    "    title=\"Momentum vs. Time at Tracker Entrance (from Dask-processed Data)\",\n",
    "    xlabel=\"Fit mom at Trk Ent [MeV/c]\",\n",
    "    ylabel=\"Fit time at Trk Ent [ns]\",\n",
    "    out_path='h2_timevmom_dask.png'\n",
    ")\n",
    "\n",
    "logger.log(\"2D histogram created: h2_timevmom_dask.png\", \"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb31e903",
   "metadata": {},
   "source": [
    "## 8. Complete Example with Multiple Files\n",
    "\n",
    "Here's a complete working example showing how to scale from a single file to many files using DaskProcessor. The same analysis code works for any number of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45655cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "COMPLETE EXAMPLE: Multi-File Analysis with DaskProcessor\n",
    "\n",
    "The following code is a template for analyzing multiple files.\n",
    "Replace file_list_path with your actual file list to run the analysis.\n",
    "\n",
    "Key features:\n",
    "- Processes multiple files in parallel using Dask\n",
    "- Same analysis code as above - just swap the Processor call\n",
    "- Automatically scales to available cores/clusters\n",
    "- Built-in progress monitoring\n",
    "\"\"\"\n",
    "\n",
    "def complete_dask_analysis(file_list_path, branches, n_workers=4):\n",
    "    \"\"\"\n",
    "    Complete workflow for multi-file analysis with DaskProcessor.\n",
    "    \n",
    "    Parameters:\n",
    "        file_list_path (str): Path to file containing list of ROOT files\n",
    "        branches (list): List of branches to extract\n",
    "        n_workers (int): Number of Dask workers to use\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results including plots and statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. INITIALIZE\n",
    "    processor = DaskProcessor(tree_path=\"EventNtuple/ntuple\", verbosity=1)\n",
    "    selector = Select(verbosity=0)\n",
    "    vector = Vector(verbosity=0)\n",
    "    plotter = Plot()\n",
    "    \n",
    "    # 2. PROCESS DATA - This is where DaskProcessor scales across files!\n",
    "    data = processor.process_data(\n",
    "        file_list_path=file_list_path,\n",
    "        branches=branches,\n",
    "        n_workers=n_workers,\n",
    "        threads_per_worker=1,\n",
    "        processes=False,\n",
    "        show_progress=True  # Progress bar during processing\n",
    "    )\n",
    "    \n",
    "    if data is None:\n",
    "        print(\"Error: No data returned from processing\")\n",
    "        return None\n",
    "    \n",
    "    # 3. APPLY SELECTIONS\n",
    "    at_trkent = selector.select_surface(data=data, surface_name=\"TT_Front\")\n",
    "    trkent = data[at_trkent]\n",
    "    \n",
    "    # 4. COMPUTE QUANTITIES\n",
    "    mom_mag = vector.get_mag(branch=trkent[\"trksegs\"], vector_name=\"mom\")\n",
    "    \n",
    "    # 5. CREATE PLOTS\n",
    "    time_flat = ak.flatten(trkent[\"trksegs\"][\"time\"], axis=None)\n",
    "    mom_mag_flat = ak.flatten(mom_mag, axis=None)\n",
    "    \n",
    "    # 1D plot\n",
    "    plotter.plot_1D(\n",
    "        time_flat, nbins=100, xmin=450, xmax=1695,\n",
    "        title=\"Time Distribution\", xlabel=\"Time [ns]\",\n",
    "        ylabel=\"Events\", out_path='time_dask.png'\n",
    "    )\n",
    "    \n",
    "    # 2D plot\n",
    "    plotter.plot_2D(\n",
    "        x=mom_mag_flat, y=time_flat,\n",
    "        nbins_x=100, xmin=85, xmax=115,\n",
    "        nbins_y=100, ymin=450, ymax=1650,\n",
    "        title=\"Momentum vs. Time\", \n",
    "        xlabel=\"Momentum [MeV/c]\", ylabel=\"Time [ns]\",\n",
    "        out_path='momentum_time_dask.png'\n",
    "    )\n",
    "    \n",
    "    # 6. RETURN RESULTS\n",
    "    return {\n",
    "        'total_events': len(data),\n",
    "        'selected_events': len(trkent),\n",
    "        'efficiency': len(trkent) / len(data),\n",
    "        'mom_min': ak.min(mom_mag_flat),\n",
    "        'mom_max': ak.max(mom_mag_flat),\n",
    "        'time_min': ak.min(time_flat),\n",
    "        'time_max': ak.max(time_flat),\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage (uncomment and modify for real analysis):\n",
    "# \n",
    "# # Create your file list (one file per line)\n",
    "# file_list = \"/path/to/file_list.txt\"\n",
    "# \n",
    "# # Run analysis\n",
    "# results = complete_dask_analysis(\n",
    "#     file_list_path=file_list,\n",
    "#     branches=[\"trksegs\"],\n",
    "#     n_workers=4\n",
    "# )\n",
    "# \n",
    "# # Print results\n",
    "# if results:\n",
    "#     print(f\"Total events: {results['total_events']}\")\n",
    "#     print(f\"Selected events: {results['selected_events']}\")\n",
    "#     print(f\"Selection efficiency: {100 * results['efficiency']:.1f}%\")\n",
    "\n",
    "logger.log(\"Complete example function defined\", \"success\")\n",
    "logger.log(\"See commented code above for usage instructions\", \"info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc8d550",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### When to Use DaskProcessor\n",
    "\n",
    "Use **DaskProcessor** (this tutorial) when:\n",
    "- ✓ Processing multiple files (10s, 100s, or 1000s)\n",
    "- ✓ You want progress monitoring across files\n",
    "- ✓ You want automatic scaling to available cores\n",
    "- ✓ You plan to connect to an HPC cluster later\n",
    "- ✓ You need resilience to transient failures\n",
    "\n",
    "Use **Standard Processor** when:\n",
    "- ✓ Working with a single file\n",
    "- ✓ Processing a small number of files (< 5)\n",
    "- ✓ Simpler debugging is needed\n",
    "- ✓ Faster startup time is important\n",
    "\n",
    "### Workflow Comparison\n",
    "\n",
    "```\n",
    "STANDARD PROCESSOR           DASK PROCESSOR\n",
    "──────────────────────────   ──────────────────────────\n",
    "File 1 → Process             File 1 ─┐\n",
    "File 2 → Process        or        File 2 ─┼→ Parallel Processing\n",
    "File 3 → Process             File 3 ─┘\n",
    "                             (scales across cores/cluster)\n",
    "\n",
    "Same analysis code after processing!\n",
    "```\n",
    "\n",
    "### Scaling Path\n",
    "\n",
    "1. **Single File**: Use standard `Processor` for development\n",
    "2. **Multiple Files**: Switch to `DaskProcessor` with local workers\n",
    "3. **Large Scale**: Point to remote Dask scheduler on HPC cluster\n",
    "4. **Code Change**: None! Just change the scheduler address\n",
    "\n",
    "The real power of DaskProcessor is that you write your analysis once, and it scales effortlessly from your laptop to a full cluster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
